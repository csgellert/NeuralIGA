{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GOp0n86adXfp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import datetime\n",
        "from copy import deepcopy as dc\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "\n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
        "    # hyperparameter.\n",
        "\n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "\n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, architecture, outermost_linear=False,\n",
        "                 first_omega_0=60, hidden_omega_0=60):\n",
        "        super().__init__()\n",
        "        self.architecture = architecture\n",
        "        in_features = architecture[0]\n",
        "        out_features = architecture[-1]\n",
        "        hidden_layers = len(architecture)-2\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.optimizer = None\n",
        "        self.name = \"SIREN\"\n",
        "        self.lr_scheduler = None\n",
        "\n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, architecture[1],\n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers-1):\n",
        "            self.net.append(SineLayer(architecture[i+1],architecture[i+2] ,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        self.net.append(SineLayer(architecture[-2],architecture[-2] ,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(architecture[-2], out_features)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / architecture[-2]) / hidden_omega_0,\n",
        "                                              np.sqrt(6 / architecture[-2]) / hidden_omega_0)\n",
        "\n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(architecture[-2], out_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "\n",
        "    def forward(self, coords):\n",
        "        #coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output\n",
        "class SIRELU(nn.Module):\n",
        "    def __init__(self, architecture, outermost_linear=False,\n",
        "                first_omega_0=60, hidden_omega_0=60):\n",
        "        super().__init__()\n",
        "        self.architecture = architecture\n",
        "        in_features = architecture[0]\n",
        "        out_features = architecture[-1]\n",
        "        hidden_layers = len(architecture)-2\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.optimizer = None\n",
        "        self.name = \"SIRELU\"\n",
        "        self.lr_scheduler = None\n",
        "\n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, architecture[1],\n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers-1):\n",
        "            if i % 2 ==0:\n",
        "                self.net.append(nn.Linear(architecture[i+1],architecture[i+2]))\n",
        "                self.net.append(nn.ReLU())\n",
        "            else:\n",
        "                self.net.append(SineLayer(architecture[i+1],architecture[i+2] ,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        if hidden_layers % 2 == 1:\n",
        "            self.net.append(nn.Linear(architecture[-2],architecture[-2]))\n",
        "            self.net.append(nn.ReLU())\n",
        "        else:\n",
        "            self.net.append(SineLayer(architecture[-2],architecture[-2] ,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(architecture[-2], out_features)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / architecture[-2]) / hidden_omega_0,\n",
        "                                              np.sqrt(6 / architecture[-2]) / hidden_omega_0)\n",
        "\n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(architecture[-2], out_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "\n",
        "    def forward(self, coords):\n",
        "        #coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def open_object(file_path):\n",
        "    vertices = []\n",
        "    faces = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('v '):\n",
        "                parts = line.strip().split()\n",
        "                vertex = list(map(float, parts[1:4]))\n",
        "                vertices.append(vertex)\n",
        "            elif line.startswith('f '):\n",
        "                parts = line.strip().split()\n",
        "                face = [int(part.split('/')[0]) - 1 for part in parts[1:4]]\n",
        "                faces.append(face)\n",
        "    return np.array(vertices), np.array(faces)\n",
        "\n",
        "def create_torch_mesh(vertices, faces, device=None):\n",
        "    if device is None:\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device('cuda')\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "    vertices_tensor = torch.tensor(vertices, dtype=torch.float32, device=device)\n",
        "    faces_tensor = torch.tensor(faces, dtype=torch.int64, device=device)\n",
        "    return vertices_tensor, faces_tensor\n",
        "\n",
        "def get_distance_from_contour(vertices, points):\n",
        "    d = torch.cdist(points, vertices)\n",
        "    min_distances, _ = torch.min(d, dim=1)\n",
        "    return min_distances\n",
        "def get_signed_distance_from_contour(vertices, faces, points):\n",
        "    \"\"\"\n",
        "    Compute signed distance from points to a 3D mesh.\n",
        "    Positive distances are inside the mesh, negative distances are outside.\n",
        "    Optimized for runtime performance.\n",
        "    \"\"\"\n",
        "    device = points.device\n",
        "    batch_size = points.shape[0]\n",
        "    \n",
        "    # Compute unsigned distance (closest distance to surface vertices)\n",
        "    # This is faster than computing distance to triangle faces for large meshes\n",
        "    d = torch.cdist(points, vertices)\n",
        "    min_distances, _ = torch.min(d, dim=1)\n",
        "    \n",
        "    # Optimized ray casting with precomputed triangle data\n",
        "    ray_direction = torch.tensor([1.0, 0.0, 0.0], device=device)\n",
        "    signs = torch.ones(batch_size, device=device)\n",
        "    \n",
        "    # Precompute triangle data once\n",
        "    face_vertices = vertices[faces]  # Shape: [num_faces, 3, 3]\n",
        "    v0 = face_vertices[:, 0]  # [num_faces, 3]\n",
        "    v1 = face_vertices[:, 1]  # [num_faces, 3]\n",
        "    v2 = face_vertices[:, 2]  # [num_faces, 3]\n",
        "    \n",
        "    # Precompute edge vectors\n",
        "    edges1 = v1 - v0  # [num_faces, 3]\n",
        "    edges2 = v2 - v0  # [num_faces, 3]\n",
        "    \n",
        "    # Precompute h vectors for all faces\n",
        "    h_vectors = torch.cross(ray_direction.unsqueeze(0).expand(len(faces), -1), edges2, dim=1)\n",
        "    determinants = torch.sum(edges1 * h_vectors, dim=1)\n",
        "    \n",
        "    # Filter out nearly parallel triangles once\n",
        "    valid_mask = torch.abs(determinants) > 1e-8\n",
        "    valid_faces_indices = torch.where(valid_mask)[0]\n",
        "    \n",
        "    if len(valid_faces_indices) == 0:\n",
        "        # All faces are parallel to ray, assume all points are outside\n",
        "        return -min_distances\n",
        "    \n",
        "    # Extract valid face data\n",
        "    valid_v0 = v0[valid_faces_indices]\n",
        "    valid_edges1 = edges1[valid_faces_indices]\n",
        "    valid_edges2 = edges2[valid_faces_indices]\n",
        "    valid_h = h_vectors[valid_faces_indices]\n",
        "    valid_det = determinants[valid_faces_indices]\n",
        "    inv_det = 1.0 / valid_det\n",
        "    \n",
        "    # Process points in batches to balance memory and speed\n",
        "    batch_size_chunk = min(1000, batch_size)\n",
        "    \n",
        "    for start_idx in range(0, batch_size, batch_size_chunk):\n",
        "        end_idx = min(start_idx + batch_size_chunk, batch_size)\n",
        "        chunk_points = points[start_idx:end_idx]\n",
        "        chunk_size = end_idx - start_idx\n",
        "        \n",
        "        # Vectorized intersection computation for this chunk\n",
        "        intersection_counts = torch.zeros(chunk_size, device=device, dtype=torch.int32)\n",
        "        \n",
        "        for i, point in enumerate(chunk_points):\n",
        "            # Compute s vectors for all valid faces\n",
        "            s_vectors = point.unsqueeze(0) - valid_v0  # [num_valid_faces, 3]\n",
        "            \n",
        "            # Compute u coordinates\n",
        "            u_coords = inv_det * torch.sum(s_vectors * valid_h, dim=1)\n",
        "            \n",
        "            # Early filtering for u coordinate bounds\n",
        "            u_valid_mask = (u_coords >= 0.0) & (u_coords <= 1.0)\n",
        "            u_valid_indices = torch.where(u_valid_mask)[0]\n",
        "            \n",
        "            if len(u_valid_indices) > 0:\n",
        "                # Compute v coordinates only for valid u\n",
        "                q_vectors = torch.cross(s_vectors[u_valid_indices], valid_edges1[u_valid_indices], dim=1)\n",
        "                v_coords = inv_det[u_valid_indices] * torch.sum(ray_direction.unsqueeze(0) * q_vectors, dim=1)\n",
        "                \n",
        "                # Check v bounds and triangle constraint\n",
        "                v_valid_mask = (v_coords >= 0.0) & (u_coords[u_valid_indices] + v_coords <= 1.0)\n",
        "                v_valid_indices = u_valid_indices[v_valid_mask]\n",
        "                \n",
        "                if len(v_valid_indices) > 0:\n",
        "                    # Compute intersection parameter t\n",
        "                    final_q = q_vectors[v_valid_mask]\n",
        "                    t_coords = inv_det[v_valid_indices] * torch.sum(valid_edges2[v_valid_indices] * final_q, dim=1)\n",
        "                    \n",
        "                    # Count intersections in front of ray\n",
        "                    intersection_counts[i] = torch.sum(t_coords > 1e-8).item()\n",
        "        \n",
        "        # Determine signs for this chunk\n",
        "        inside_mask = (intersection_counts % 2) == 1\n",
        "        signs[start_idx:end_idx] = torch.where(inside_mask, 1.0, -1.0)\n",
        "    \n",
        "    return signs * min_distances\n",
        "def train_model(model, optimizer, num_epoch, lr_scheduler = None):\n",
        "    geom_model = open_object(\"./3D_model_data/stanford-bunny.obj\")\n",
        "    vertices, faces = geom_model\n",
        "    vertices_tensor, faces_tensor = create_torch_mesh(vertices, faces)\n",
        "    print(\"Model loaded successfully\")\n",
        "    criterion = nn.L1Loss()\n",
        "    loss_history = []\n",
        "    steps_til_summary = num_epoch/10\n",
        "    for epoch in range(num_epoch):\n",
        "        optimizer.zero_grad()\n",
        "        # Dummy input and target for illustration purposes\n",
        "        input_points = (torch.rand((30000, 3))-0.5)*0.2\n",
        "        input_points = input_points.cuda()\n",
        "        target_distances = get_signed_distance_from_contour(vertices_tensor.cuda(), faces_tensor.cuda(), input_points).unsqueeze(1)\n",
        "        predicted_distances = model(input_points)\n",
        "        loss = criterion(predicted_distances, target_distances)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % steps_til_summary == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "        loss_history.append(loss.item())\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "        lr_scheduler.step()\n",
        "    plt.plot(loss_history)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.show()\n",
        "    return model\n",
        "def train_two_models(model1, model2,num_epoch=100):\n",
        "    geom_model = open_object(\"stanford-bunny.obj\")\n",
        "    vertices, faces = geom_model\n",
        "    vertices_tensor, faces_tensor = create_torch_mesh(vertices, faces)\n",
        "    print(\"Model loaded successfully\")\n",
        "    criterion = nn.L1Loss()\n",
        "    steps_til_summary = num_epoch/10\n",
        "    for epoch in range(num_epoch):\n",
        "        model1.optimizer.zero_grad()\n",
        "        model2.optimizer.zero_grad()\n",
        "        # Dummy input and target for illustration purposes\n",
        "        input_points = (torch.rand((30000, 3))-0.5)*0.2\n",
        "        input_points = input_points.cuda()\n",
        "        target_distances = get_signed_distance_from_contour(vertices_tensor.cuda(), faces_tensor.cuda(), input_points).unsqueeze(1)\n",
        "        predicted_distances1 = model(input_points)\n",
        "        predicted_distances2 = model2(input_points)\n",
        "        loss1 = criterion(predicted_distances1, target_distances)\n",
        "        loss2 = criterion(predicted_distances2, target_distances)\n",
        "        loss1.backward()\n",
        "        loss2.backward()\n",
        "        model1.optimizer.step()\n",
        "        model2.optimizer.step()\n",
        "        if epoch % steps_til_summary == 0:\n",
        "            print(f'Epoch {epoch}, Loss1: {loss1.item()}, Loss2: {loss2.item()}')\n",
        "        model1.loss_history.append(loss1.item())\n",
        "        model2.loss_history.append(loss2.item())\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "        model1.lr_scheduler.step()\n",
        "        model2.lr_scheduler.step()\n",
        "def plot2model_loss(model1,model2):\n",
        "    plt.plot(model1.loss_history)\n",
        "    plt.plot(model2.loss_history)\n",
        "    plt.legend(['model1','model2'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yscale('log')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.show()\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m architecture \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSiren\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutermost_linear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_omega_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_omega_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;241m.\u001b[39mrequires_grad)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Egyetem\\Kutatás\\NeuralIGA\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:309\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "architecture = [3,256,256,256,256,1]\n",
        "model = Siren(architecture, outermost_linear=True, first_omega_0=60, hidden_omega_0=60)\n",
        "print(f\"number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "model.optimizer = torch.optim.Adam(lr=1e-6, params=model.parameters())\n",
        "model.lr_scheduler = torch.optim.lr_scheduler.StepLR(model.optimizer, step_size=100, gamma=0.7)\n",
        "\n",
        "model2 = SIRELU(architecture=architecture,outermost_linear=True,first_omega_0=60,hidden_omega_0=60)\n",
        "print(f\"number of parameters: {sum(p.numel() for p in model2.parameters() if p.requires_grad)}\")\n",
        "model2.optimizer = torch.optim.Adam(lr=1e-6, params=model2.parameters())\n",
        "model2.lr_scheduler = torch.optim.lr_scheduler.StepLR(model2.optimizer, step_size=100, gamma=0.7)\n",
        "print(model2.net)\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
